import json
import torch
import numpy as np
import torch.nn.functional as F
from torch import Tensor
from transformers import AutoTokenizer, AutoModel

# --------------------------
# 1. æ¨¡å‹å’Œè·¯å¾„é…ç½®ï¼ˆéœ€ä¿®æ”¹ä¸ºä½ çš„æœ¬åœ°è·¯å¾„ï¼‰
# --------------------------
# æœ¬åœ°Qwen3-Embeddingæ¨¡å‹è·¯å¾„ï¼ˆæ›¿æ¢ä¸ºä½ ä¸‹è½½çš„æ¨¡å‹ç›®å½•ï¼‰
local_model_path = "/home/hww/lhf/SigmaDiff-main/IR_description/Qwen3-Embedding-0.6B/"
# IRè¯­ä¹‰æè¿°æ–‡ä»¶è·¯å¾„ï¼ˆç¡®ä¿ä¸è„šæœ¬åŒç›®å½•æˆ–å¡«å†™ç»å¯¹è·¯å¾„ï¼‰
semantics_file = "llvm_ir_semantics.json"
# åµŒå…¥ç»“æœä¿å­˜è·¯å¾„
output_file = "llvm_ir_embeddings_qwen.json"


# --------------------------
# 2. Qwenæ¨¡å‹è¦æ±‚çš„æ± åŒ–å‡½æ•°
# --------------------------
def last_token_pool(last_hidden_states: Tensor, attention_mask: Tensor) -> Tensor:
    """æ¨¡å‹å®˜æ–¹è¦æ±‚çš„last tokenæ± åŒ–é€»è¾‘"""
    left_padding = (attention_mask[:, -1].sum() == attention_mask.shape[0])
    if left_padding:
        return last_hidden_states[:, -1]
    else:
        sequence_lengths = attention_mask.sum(dim=1) - 1
        batch_size = last_hidden_states.shape[0]
        return last_hidden_states[
            torch.arange(batch_size, device=last_hidden_states.device),
            sequence_lengths
        ]


# --------------------------
# 3. åŠ è½½æ•°æ®å’Œæ¨¡å‹
# --------------------------
# è¯»å–IRæŒ‡ä»¤è¯­ä¹‰æè¿°
with open(semantics_file, "r", encoding="utf-8") as f:
    ir_semantics = json.load(f)

# æå–æŒ‡ä»¤åå’Œå¯¹åº”çš„è¯­ä¹‰æ–‡æœ¬ï¼ˆä¿æŒé¡ºåºï¼‰
instr_names = list(ir_semantics.keys())
instr_texts = [ir_semantics[name] for name in instr_names]

# åŠ è½½åˆ†è¯å™¨å’Œæ¨¡å‹ï¼ˆæœ¬åœ°è·¯å¾„ï¼Œå¼ºåˆ¶CPUé¿å…CUDAé—®é¢˜ï¼‰
tokenizer = AutoTokenizer.from_pretrained(
    local_model_path,
    padding_side="left"  # æ¨¡å‹è¦æ±‚left padding
)
model = AutoModel.from_pretrained(local_model_path).to("cpu")  # å¼ºåˆ¶ç”¨CPU
model.eval()


# --------------------------
# 4. ç”ŸæˆåµŒå…¥å‘é‡
# --------------------------
def generate_embeddings(texts, batch_size=8):
    """æ‰¹é‡ç”ŸæˆåµŒå…¥ï¼Œé€‚é…é•¿æ–‡æœ¬åˆ—è¡¨"""
    all_embeddings = []
    for i in range(0, len(texts), batch_size):
        batch_texts = texts[i:i + batch_size]

        # Tokenizeï¼ˆæ¨¡å‹è¦æ±‚max_length=8192ï¼‰
        batch_dict = tokenizer(
            batch_texts,
            padding=True,
            truncation=True,
            max_length=8192,
            return_tensors="pt"
        ).to(model.device)  # è¾“å…¥åˆ°æ¨¡å‹æ‰€åœ¨è®¾å¤‡ï¼ˆCPUï¼‰

        # æ¨¡å‹å‰å‘ä¼ æ’­
        with torch.no_grad():
            outputs = model(**batch_dict)

        # æ± åŒ–+å½’ä¸€åŒ–ï¼ˆæ¨¡å‹å®˜æ–¹è¦æ±‚ï¼‰
        embeddings = last_token_pool(outputs.last_hidden_state, batch_dict["attention_mask"])
        embeddings = F.normalize(embeddings, p=2, dim=1)  # L2å½’ä¸€åŒ–

        # è½¬æ¢ä¸ºåˆ—è¡¨å¹¶æ”¶é›†
        all_embeddings.extend(embeddings.cpu().numpy().tolist())

    return all_embeddings


# æ‰§è¡Œç”Ÿæˆ
print(f"å¼€å§‹ç”ŸæˆåµŒå…¥ï¼Œå…±{len(instr_texts)}æ¡IRæŒ‡ä»¤...")
ir_embeddings = generate_embeddings(instr_texts)

# æ˜ å°„å›æŒ‡ä»¤å
result = {instr_names[i]: ir_embeddings[i] for i in range(len(instr_names))}

# --------------------------
# 5. ä¿å­˜ç»“æœ
# --------------------------
with open(output_file, "w", encoding="utf-8") as f:
    json.dump(result, f, indent=2)

print(f"ğŸ‰ åµŒå…¥ç”Ÿæˆå®Œæˆï¼å·²ä¿å­˜è‡³ {output_file}ï¼ˆç»´åº¦ï¼š{len(ir_embeddings[0])}ï¼‰")